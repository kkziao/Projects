{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5abbbe63",
   "metadata": {},
   "source": [
    "# 　　　　`Convolutional Neural Network` for Deep Learning\n",
    "# 　　　　　　　　on Handwritten Digits `Classification` Task"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "90c3cbb9",
   "metadata": {},
   "source": [
    "## Task Overview \n",
    "<img src=\"https://user-images.githubusercontent.com/68801296/88917938-4008f180-d286-11ea-8667-50027700e3ea.png\" width=\"500\">\n",
    "　　　　　　　　　　　　　　　　　　　　　　　　　　　　<strong>A Task Schema Diagram</strong>\n",
    "                                              "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9883b4ba",
   "metadata": {},
   "source": [
    "## Setup Development Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a47d22f8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Import related Modules for TensorFlow framework and Keras API\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "from tensorflow.keras import layers, optimizers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5ec08005",
   "metadata": {},
   "source": [
    "## Prepare Dataset \n",
    "\n",
    "### The Image Dataset of MNIST Handwritten Digits - Introduction "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7cca8177",
   "metadata": {},
   "source": [
    "### Load Dataset\n",
    "\n",
    "Load the MNIST dataset distributed with Keras.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a4ac2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the MNIST dataset \n",
    "mnist_dataset = keras.datasets.mnist\n",
    "\n",
    "## Split MNIST dataset to get the images and labels for both train and test sets \n",
    "(images_train, labels_train),(images_test, labels_test) = mnist_dataset.load_data()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a4fb690f",
   "metadata": {},
   "source": [
    "###  Explore Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2f7093ad",
   "metadata": {},
   "source": [
    "##### Find out dimensions of the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91a3909d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of train dataset: \n",
      " (60000, 28, 28)\n",
      "Dimension of test dataset: \n",
      " (10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "images_train_dimension = images_train.shape\n",
    "print(\"Dimension of train dataset: \\n\", images_train_dimension)\n",
    "\n",
    "images_test_dimension = images_test.shape\n",
    "print(\"Dimension of test dataset: \\n\", images_test_dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61003758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train images: \n",
      " 60000\n",
      "Dimension of an image: \n",
      " (28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of train images: \\n\", images_train_dimension[0])\n",
    "\n",
    "print(\"Dimension of an image: \\n\", images_train_dimension[1:3])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1e04cd62",
   "metadata": {},
   "source": [
    "##### Find out the range of pixel values in image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee50e3fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The range of pixel values of the images is:  [0, 255]\n"
     ]
    }
   ],
   "source": [
    "##-Q5: What is the value range of all image pixels in the dataset?\n",
    "print( \"The range of pixel values of the images is: \", [np.min(images_train), np.max(images_train)] )  \n",
    " "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "59b9364a",
   "metadata": {},
   "source": [
    "##### Find out the value of labels and their frequencies  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "277d4b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The unique values of the labels in the dataset are: \n",
      " [0 1 2 3 4 5 6 7 8 9]\n",
      "\n",
      "\n",
      "The count of each label in the dataset are: \n",
      " [5923 6742 5958 6131 5842 5421 5918 6265 5851 5949]\n"
     ]
    }
   ],
   "source": [
    "##-Q6: How many value of labels for the images in the dataset?\n",
    "(labels, counts) = np.unique(labels_train, return_counts=True)\n",
    "print( \"The unique values of the labels in the dataset are: \\n\", labels )\n",
    "print('\\n')\n",
    "print( \"The count of each label in the dataset are: \\n\", counts )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "120360dd",
   "metadata": {},
   "source": [
    "##### Display a sample image \n",
    "get an idea of the kind of images your CNN is going to work with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fb0f2d2a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image of the 6th in train dataset is displayed below: \n",
      "\n",
      "The label for the second training image is: \n",
      " 2\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaEAAAGdCAYAAAC7EMwUAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAa00lEQVR4nO3df2zU9R3H8dcB5QS8XtJAe9eBXaMQmSVEwAGNIprQ0W2MH9uCSkwxEX/ww7DqVEYWqkuokkhc1omZ2ZhmMvhDZCQypQZacMiCBCNhSsosows0DQTvSsFrgM/+IFx2thY+x13fvfb5SD4J9/1+3/2++fJNX3z6vfs04JxzAgDAwCDrBgAAAxchBAAwQwgBAMwQQgAAM4QQAMAMIQQAMEMIAQDMEEIAADNDrBv4psuXL+vkyZMKhUIKBALW7QAAPDnn1N7eruLiYg0a1PNcp8+F0MmTJzVmzBjrNgAAN6ilpUWjR4/u8Zg+9+O4UChk3QIAIAOu5/t51kLotddeU2lpqW666SZNnjxZe/fuva46fgQHAP3D9Xw/z0oIbdmyRStXrtTq1at16NAh3XPPPaqsrNSJEyeycToAQI4KZGMV7alTp2rSpEnasGFDctv48eM1b9481dbW9lgbj8cVDocz3RIAoJfFYjHl5+f3eEzGZ0KdnZ06ePCgKioqUrZXVFRo3759XY5PJBKKx+MpAwAwMGQ8hE6fPq1Lly6pqKgoZXtRUZFaW1u7HF9bW6twOJwcvDMOAAaOrL0x4ZsPpJxz3T6kWrVqlWKxWHK0tLRkqyUAQB+T8c8JjRw5UoMHD+4y62lra+syO5KkYDCoYDCY6TYAADkg4zOhoUOHavLkyaqvr0/ZXl9fr/Ly8kyfDgCQw7KyYkJ1dbUefvhhTZkyRdOnT9cf/vAHnThxQk888UQ2TgcAyFFZCaGFCxfqzJkzevHFF3Xq1CmVlZVpx44dKikpycbpAAA5KiufE7oRfE4IAPoHk88JAQBwvQghAIAZQggAYIYQAgCYIYQAAGYIIQCAGUIIAGCGEAIAmCGEAABmCCEAgBlCCABghhACAJghhAAAZgghAIAZQggAYIYQAgCYIYQAAGYIIQCAGUIIAGCGEAIAmCGEAABmCCEAgBlCCABghhACAJghhAAAZgghAIAZQggAYIYQAgCYIYQAAGYIIQCAGUIIAGCGEAIAmCGEAABmCCEAgBlCCABghhACAJghhAAAZgghAIAZQggAYIYQAgCYIYQAAGYIIQCAGUIIAGCGEAIAmCGEAABmCCEAgBlCCABghhACAJghhAAAZgghAIAZQggAYIYQAgCYGWLdAIDrEwqFvGtuvvnmtM71ox/9yLtm1KhR3jXr16/3rkkkEt416LuYCQEAzBBCAAAzGQ+hmpoaBQKBlBGJRDJ9GgBAP5CVZ0J33HGHPvzww+TrwYMHZ+M0AIAcl5UQGjJkCLMfAMA1ZeWZUFNTk4qLi1VaWqoHHnhAX3755bcem0gkFI/HUwYAYGDIeAhNnTpVb731lj744AO98cYbam1tVXl5uc6cOdPt8bW1tQqHw8kxZsyYTLcEAOijAs45l80TdHR06NZbb9Wzzz6r6urqLvsTiUTK+/7j8ThBBHSDzwldweeEckcsFlN+fn6Px2T9w6ojRozQhAkT1NTU1O3+YDCoYDCY7TYAAH1Q1j8nlEgk9PnnnysajWb7VACAHJPxEHrmmWfU2Nio5uZm/fOf/9TPfvYzxeNxVVVVZfpUAIAcl/Efx/33v//Vgw8+qNOnT2vUqFGaNm2a9u/fr5KSkkyfCgCQ4zIeQps3b870lwT6tO9+97veNc8995x3zfTp071rysrKvGt6Uzo/pn/qqaey0AmssHYcAMAMIQQAMEMIAQDMEEIAADOEEADADCEEADBDCAEAzBBCAAAzhBAAwAwhBAAwQwgBAMwQQgAAM1n/zaq+4vG4wuGwdRvIcbfffntadStXrvSuWbRokXfNsGHDvGsCgYB3TUtLi3eNJLW3t3vXjB8/3rvm9OnT3jUzZ870rvniiy+8a3Djruc3qzITAgCYIYQAAGYIIQCAGUIIAGCGEAIAmCGEAABmCCEAgBlCCABghhACAJghhAAAZgghAIAZQggAYIYQAgCYGWLdAAaWdFZIf/nll71rFi5c6F0jSaFQKK263tDU1ORd84Mf/CCtc+Xl5XnXpLNS9ciRI3ulBn0XMyEAgBlCCABghhACAJghhAAAZgghAIAZQggAYIYQAgCYIYQAAGYIIQCAGUIIAGCGEAIAmCGEAABmWMAUvWr+/PneNY8++mgWOrH173//27tm1qxZ3jUtLS3eNZJ02223pVUH+GImBAAwQwgBAMwQQgAAM4QQAMAMIQQAMEMIAQDMEEIAADOEEADADCEEADBDCAEAzBBCAAAzhBAAwAwLmKJX/fznP7duoUfHjx/3rjlw4IB3zXPPPeddk+5ipOkYP358r50LAxszIQCAGUIIAGDGO4T27NmjOXPmqLi4WIFAQNu2bUvZ75xTTU2NiouLNWzYMM2cOVNHjhzJVL8AgH7EO4Q6Ojo0ceJE1dXVdbt/3bp1Wr9+verq6nTgwAFFIhHNmjVL7e3tN9wsAKB/8X5jQmVlpSorK7vd55zTq6++qtWrV2vBggWSpDfffFNFRUXatGmTHn/88RvrFgDQr2T0mVBzc7NaW1tVUVGR3BYMBnXvvfdq37593dYkEgnF4/GUAQAYGDIaQq2trZKkoqKilO1FRUXJfd9UW1urcDicHGPGjMlkSwCAPiwr744LBAIpr51zXbZdtWrVKsViseTozc9CAABsZfTDqpFIRNKVGVE0Gk1ub2tr6zI7uioYDCoYDGayDQBAjsjoTKi0tFSRSET19fXJbZ2dnWpsbFR5eXkmTwUA6Ae8Z0Lnzp3TsWPHkq+bm5v16aefqqCgQLfccotWrlyptWvXauzYsRo7dqzWrl2r4cOH66GHHspo4wCA3OcdQp988onuu+++5Ovq6mpJUlVVlf785z/r2Wef1YULF7R06VKdPXtWU6dO1c6dOxUKhTLXNQCgXwg455x1E/8vHo8rHA5bt4EsKS4u9q557LHHvGt27tzpXSMpZZZ/vdra2tI6V1/26KOPete8/vrrWeikq5kzZ3rXfPTRR5lvBNcUi8WUn5/f4zGsHQcAMEMIAQDMEEIAADOEEADADCEEADBDCAEAzBBCAAAzhBAAwAwhBAAwQwgBAMwQQgAAM4QQAMAMIQQAMJPR36wKXMvJkye9a2pqajLfCHo0ffp06xYwQDATAgCYIYQAAGYIIQCAGUIIAGCGEAIAmCGEAABmCCEAgBlCCABghhACAJghhAAAZgghAIAZQggAYIYFTIEb9NRTT3nXjBgxIgudZM6ECRN65Tz79u3zrvn444+z0AmsMBMCAJghhAAAZgghAIAZQggAYIYQAgCYIYQAAGYIIQCAGUIIAGCGEAIAmCGEAABmCCEAgBlCCABghgVM0ecNHz7cu+Z73/teWudas2aNd80Pf/jDtM7la9Ag//8zXr58OQuddO/kyZPeNY888oh3zaVLl7xr0HcxEwIAmCGEAABmCCEAgBlCCABghhACAJghhAAAZgghAIAZQggAYIYQAgCYIYQAAGYIIQCAGUIIAGCGBUyRtry8PO+aO++807vmnXfe8a6JRqPeNZJ04cIF75p0Fu78+OOPvWtmz57tXZPO4q/pGjLE/9vJggULvGt++9vfetd0dnZ616B3MBMCAJghhAAAZrxDaM+ePZozZ46Ki4sVCAS0bdu2lP2LFy9WIBBIGdOmTctUvwCAfsQ7hDo6OjRx4kTV1dV96zGzZ8/WqVOnkmPHjh031CQAoH/yfpJYWVmpysrKHo8JBoOKRCJpNwUAGBiy8kyooaFBhYWFGjdunJYsWaK2trZvPTaRSCgej6cMAMDAkPEQqqys1Ntvv61du3bplVde0YEDB3T//fcrkUh0e3xtba3C4XByjBkzJtMtAQD6qIx/TmjhwoXJP5eVlWnKlCkqKSnRe++91+1nAlatWqXq6urk63g8ThABwACR9Q+rRqNRlZSUqKmpqdv9wWBQwWAw220AAPqgrH9O6MyZM2ppaUn7E+wAgP7LeyZ07tw5HTt2LPm6ublZn376qQoKClRQUKCamhr99Kc/VTQa1fHjx/WrX/1KI0eO1Pz58zPaOAAg93mH0CeffKL77rsv+frq85yqqipt2LBBhw8f1ltvvaWvvvpK0WhU9913n7Zs2aJQKJS5rgEA/ULAOeesm/h/8Xhc4XDYuo0BZejQoWnVpbOg5tatW9M6l68XXnghrbpdu3Z51/zjH//wrikoKPCuSae3srIy75q+btGiRd4131zZ5Xp927t6cX1isZjy8/N7PIa14wAAZgghAIAZQggAYIYQAgCYIYQAAGYIIQCAGUIIAGCGEAIAmCGEAABmCCEAgBlCCABghhACAJghhAAAZlhFu5/Jy8vzrnnxxRfTOtcvf/nLtOp8/f3vf/euefjhh9M611dffeVdM2rUKO+aHTt2eNdMmjTJu6azs9O7RpLWrVvnXZPOit1z5871rknHhx9+mFbdyy+/7F1z9uzZtM7l69NPP+2V89wIVtEGAPRphBAAwAwhBAAwQwgBAMwQQgAAM4QQAMAMIQQAMEMIAQDMEEIAADOEEADADCEEADBDCAEAzAyxbgDfbvDgwd41v/nNb7xrnnnmGe8aSero6PCuef75571rNm/e7F2TzkKkkjRlyhTvmrq6Ou+aO++807umqanJu+bJJ5/0rpGk3bt3e9dca6HK7pSXl3vXLFq0yLvmJz/5iXeNJNXX16dV56ulpcW7prS0NAud9D5mQgAAM4QQAMAMIQQAMEMIAQDMEEIAADOEEADADCEEADBDCAEAzBBCAAAzhBAAwAwhBAAwQwgBAMwEnHPOuon/F4/HFQ6HrdvoE9JZfPJ3v/udd8358+e9ayTpscce867ZuXOnd83UqVO9ax555BHvGkmqrKz0rhk2bJh3zYsvvuhds3HjRu+adBbG7I8efPDBtOoeeuihDHfSvV/84hfeNceOHctCJ5kVi8WuubAtMyEAgBlCCABghhACAJghhAAAZgghAIAZQggAYIYQAgCYIYQAAGYIIQCAGUIIAGCGEAIAmCGEAABmWMC0Dzt16pR3zahRo7xrEomEd40kffHFF941I0aM8K657bbbvGt6U01NjXdNbW2td82lS5e8awBLLGAKAOjTCCEAgBmvEKqtrdVdd92lUCikwsJCzZs3T0ePHk05xjmnmpoaFRcXa9iwYZo5c6aOHDmS0aYBAP2DVwg1NjZq2bJl2r9/v+rr63Xx4kVVVFSoo6Mjecy6deu0fv161dXV6cCBA4pEIpo1a5ba29sz3jwAILcN8Tn4/fffT3m9ceNGFRYW6uDBg5oxY4acc3r11Ve1evVqLViwQJL05ptvqqioSJs2bdLjjz+euc4BADnvhp4JxWIxSVJBQYEkqbm5Wa2traqoqEgeEwwGde+992rfvn3dfo1EIqF4PJ4yAAADQ9oh5JxTdXW17r77bpWVlUmSWltbJUlFRUUpxxYVFSX3fVNtba3C4XByjBkzJt2WAAA5Ju0QWr58uT777DP99a9/7bIvEAikvHbOddl21apVqxSLxZKjpaUl3ZYAADnG65nQVStWrND27du1Z88ejR49Ork9EolIujIjikajye1tbW1dZkdXBYNBBYPBdNoAAOQ4r5mQc07Lly/X1q1btWvXLpWWlqbsLy0tVSQSUX19fXJbZ2enGhsbVV5enpmOAQD9htdMaNmyZdq0aZP+9re/KRQKJZ/zhMNhDRs2TIFAQCtXrtTatWs1duxYjR07VmvXrtXw4cP10EMPZeUvAADIXV4htGHDBknSzJkzU7Zv3LhRixcvliQ9++yzunDhgpYuXaqzZ89q6tSp2rlzp0KhUEYaBgD0Hyxg2ocdOnTIu2bChAlZ6MTWjh07vGv27NmT1rm2bdvmXXP8+HHvmosXL3rXALmGBUwBAH0aIQQAMEMIAQDMEEIAADOEEADADCEEADBDCAEAzBBCAAAzhBAAwAwhBAAwQwgBAMwQQgAAM4QQAMBMWr9ZFb1jxowZ3jXz5s3zrpk0aZJ3jXTlN+b6+tOf/uRdc/bsWe+azs5O7xoAvY+ZEADADCEEADBDCAEAzBBCAAAzhBAAwAwhBAAwQwgBAMwQQgAAM4QQAMAMIQQAMEMIAQDMEEIAADMB55yzbuL/xeNxhcNh6zYAADcoFospPz+/x2OYCQEAzBBCAAAzhBAAwAwhBAAwQwgBAMwQQgAAM4QQAMAMIQQAMEMIAQDMEEIAADOEEADADCEEADBDCAEAzBBCAAAzhBAAwAwhBAAwQwgBAMwQQgAAM4QQAMAMIQQAMEMIAQDMEEIAADOEEADADCEEADBDCAEAzBBCAAAzhBAAwAwhBAAw4xVCtbW1uuuuuxQKhVRYWKh58+bp6NGjKccsXrxYgUAgZUybNi2jTQMA+gevEGpsbNSyZcu0f/9+1dfX6+LFi6qoqFBHR0fKcbNnz9apU6eSY8eOHRltGgDQPwzxOfj9999Peb1x40YVFhbq4MGDmjFjRnJ7MBhUJBLJTIcAgH7rhp4JxWIxSVJBQUHK9oaGBhUWFmrcuHFasmSJ2travvVrJBIJxePxlAEAGBgCzjmXTqFzTnPnztXZs2e1d+/e5PYtW7bo5ptvVklJiZqbm/XrX/9aFy9e1MGDBxUMBrt8nZqaGr3wwgvp/w0AAH1SLBZTfn5+zwe5NC1dutSVlJS4lpaWHo87efKky8vLc++88063+7/++msXi8WSo6WlxUliMBgMRo6PWCx2zSzxeiZ01YoVK7R9+3bt2bNHo0eP7vHYaDSqkpISNTU1dbs/GAx2O0MCAPR/XiHknNOKFSv07rvvqqGhQaWlpdesOXPmjFpaWhSNRtNuEgDQP3m9MWHZsmX6y1/+ok2bNikUCqm1tVWtra26cOGCJOncuXN65pln9PHHH+v48eNqaGjQnDlzNHLkSM2fPz8rfwEAQA7zeQ6kb/m538aNG51zzp0/f95VVFS4UaNGuby8PHfLLbe4qqoqd+LEies+RywWM/85JoPBYDBufFzPM6G03x2XLfF4XOFw2LoNAMANup53x7F2HADADCEEADBDCAEAzBBCAAAzhBAAwAwhBAAwQwgBAMwQQgAAM4QQAMAMIQQAMEMIAQDMEEIAADOEEADADCEEADBDCAEAzBBCAAAzhBAAwAwhBAAwQwgBAMwQQgAAM4QQAMAMIQQAMEMIAQDMEEIAADOEEADATJ8LIeecdQsAgAy4nu/nfS6E2tvbrVsAAGTA9Xw/D7g+NvW4fPmyTp48qVAopEAgkLIvHo9rzJgxamlpUX5+vlGH9rgOV3AdruA6XMF1uKIvXAfnnNrb21VcXKxBg3qe6wzppZ6u26BBgzR69Ogej8nPzx/QN9lVXIcruA5XcB2u4DpcYX0dwuHwdR3X534cBwAYOAghAICZnAqhYDCoNWvWKBgMWrdiiutwBdfhCq7DFVyHK3LtOvS5NyYAAAaOnJoJAQD6F0IIAGCGEAIAmCGEAABmciqEXnvtNZWWluqmm27S5MmTtXfvXuuWelVNTY0CgUDKiEQi1m1l3Z49ezRnzhwVFxcrEAho27ZtKfudc6qpqVFxcbGGDRummTNn6siRIzbNZtG1rsPixYu73B/Tpk2zaTZLamtrdddddykUCqmwsFDz5s3T0aNHU44ZCPfD9VyHXLkfciaEtmzZopUrV2r16tU6dOiQ7rnnHlVWVurEiRPWrfWqO+64Q6dOnUqOw4cPW7eUdR0dHZo4caLq6uq63b9u3TqtX79edXV1OnDggCKRiGbNmtXv1iG81nWQpNmzZ6fcHzt27OjFDrOvsbFRy5Yt0/79+1VfX6+LFy+qoqJCHR0dyWMGwv1wPddBypH7weWI73//++6JJ55I2Xb77be7559/3qij3rdmzRo3ceJE6zZMSXLvvvtu8vXly5ddJBJxL730UnLb119/7cLhsHv99dcNOuwd37wOzjlXVVXl5s6da9KPlba2NifJNTY2OucG7v3wzevgXO7cDzkxE+rs7NTBgwdVUVGRsr2iokL79u0z6spGU1OTiouLVVpaqgceeEBffvmldUummpub1dramnJvBINB3XvvvQPu3pCkhoYGFRYWaty4cVqyZIna2tqsW8qqWCwmSSooKJA0cO+Hb16Hq3LhfsiJEDp9+rQuXbqkoqKilO1FRUVqbW016qr3TZ06VW+99ZY++OADvfHGG2ptbVV5ebnOnDlj3ZqZq//+A/3ekKTKykq9/fbb2rVrl1555RUdOHBA999/vxKJhHVrWeGcU3V1te6++26VlZVJGpj3Q3fXQcqd+6HPraLdk2/+agfnXJdt/VllZWXyzxMmTND06dN166236s0331R1dbVhZ/YG+r0hSQsXLkz+uaysTFOmTFFJSYnee+89LViwwLCz7Fi+fLk+++wzffTRR132DaT74duuQ67cDzkxExo5cqQGDx7c5X8ybW1tXf7HM5CMGDFCEyZMUFNTk3UrZq6+O5B7o6toNKqSkpJ+eX+sWLFC27dv1+7du1N+9ctAux++7Tp0p6/eDzkRQkOHDtXkyZNVX1+fsr2+vl7l5eVGXdlLJBL6/PPPFY1GrVsxU1paqkgkknJvdHZ2qrGxcUDfG5J05swZtbS09Kv7wzmn5cuXa+vWrdq1a5dKS0tT9g+U++Fa16E7ffZ+MHxThJfNmze7vLw898c//tH961//citXrnQjRoxwx48ft26t1zz99NOuoaHBffnll27//v3uxz/+sQuFQv3+GrS3t7tDhw65Q4cOOUlu/fr17tChQ+4///mPc865l156yYXDYbd161Z3+PBh9+CDD7poNOri8bhx55nV03Vob293Tz/9tNu3b59rbm52u3fvdtOnT3ff+c53+tV1ePLJJ104HHYNDQ3u1KlTyXH+/PnkMQPhfrjWdcil+yFnQsg5537/+9+7kpISN3ToUDdp0qSUtyMOBAsXLnTRaNTl5eW54uJit2DBAnfkyBHrtrJu9+7dTlKXUVVV5Zy78rbcNWvWuEgk4oLBoJsxY4Y7fPiwbdNZ0NN1OH/+vKuoqHCjRo1yeXl57pZbbnFVVVXuxIkT1m1nVHd/f0lu48aNyWMGwv1wreuQS/cDv8oBAGAmJ54JAQD6J0IIAGCGEAIAmCGEAABmCCEAgBlCCABghhACAJghhAAAZgghAIAZQggAYIYQAgCYIYQAAGb+B9MSKSMfO5goAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Display the 6th image from the train dataset \n",
    "\n",
    "idx_img = 5\n",
    "\n",
    "print(\"The image of the 6th in train dataset is displayed below: \\n\")\n",
    "\n",
    "## If encounter error message that says package matplotlib is not found, then install it by running code:  \n",
    "##    pip install matplotlib   \n",
    "\n",
    "# pip install matplotlib   \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.gray()   ## Set the colormap for showing the image as grayscale\n",
    "plt.imshow(images_train[idx_img])\n",
    "\n",
    "print(\"The label for the second training image is: \\n\", labels_train[idx_img])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1eb29880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The digit written in the 6th image is:  2\n"
     ]
    }
   ],
   "source": [
    "## Based on the returned results from executing above code to explore the sample image, answer in the Course Shell: \n",
    "\n",
    "print(\"The digit written in the 6th image is: \", labels_train[idx_img])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "04bbc8b5",
   "metadata": {},
   "source": [
    "### Preprocess the data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "69f084a6",
   "metadata": {},
   "source": [
    "Create some functions that can be used to process the dataset as demanded before feed it in to train the network parameters.  \n",
    "\n",
    "The tensor of shape of an image's dimension is more formally described as (image_height, image_width, color_channels) instead of just (image_height, image_width), where color_channels refers to (R,G,B). For example, for one color image that has 3 color channels, its dimension would be (image_height, image_width, 3) \n",
    "\n",
    "Since the MNIST dataset contains images all in grayscale, so we can explicitly express that in its dimension for model training, by reshaping and adding the number of channels.\n",
    "\n",
    "We also need to map the image pixel value range from its original to the range of 0 to 1 in the float32 type to facilitate the parameter computations.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4dc1f5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define a function that convert and rescale values of images and labels        \n",
    "\n",
    "def grayscale_converter(image, label):\n",
    "    \n",
    "    image = tf.cast(image, tf.float32)    ## Convert to enforce the values of image in floating-point number data type\n",
    "    label = tf.cast(label, tf.int64)      ## Convert to enforce the values of label in integer number data type\n",
    "    \n",
    "    image = image / 255  ## Rescale value of image pixels to a range of [0, 1] as gray level\n",
    "    \n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ca63f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define a function that enforce the image color channel for grayscale image, \n",
    "##  and randomly sample the amount from preprocessed images and labels to prepare dataset that suit for the model        \n",
    "\n",
    "def preprocess_dataset(images, labels, count, batch_size):       \n",
    "    \n",
    "    ## Convert dimension of all images to enforce the grayscale of the dataset that has only one color channel\n",
    "    images_dimension = images.shape\n",
    "    images = images.reshape(images_dimension[0], images_dimension[1], images_dimension[2], 1)        \n",
    "     \n",
    "    ## Prepare dataset by packing randomly sampled and batched images and lables\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((images, labels))    \n",
    "    dataset = dataset.map(grayscale_converter)    ## rescale images in dataset to a range of 0 to 1\n",
    "    dataset = dataset.take(count).shuffle(count).batch(batch_size)  ## randomly sample and batch the dataset\n",
    "    \n",
    "    return dataset\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "94e708e9",
   "metadata": {},
   "source": [
    "# Build a new Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2bef49ed",
   "metadata": {},
   "source": [
    "##### Stack and Configure layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "361e5770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Pro\n",
      "\n",
      "systemMemory: 16.00 GB\n",
      "maxCacheSize: 5.33 GB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Build a tf.keras.Sequential model by stacking layers\n",
    "\n",
    "model = keras.Sequential([     \n",
    "    \n",
    "    ## A Convolutional layer that \n",
    "    ##   receive image input with tensors of shape (image_height, image_width, color_channels), \n",
    "    ##   has 32 filters, \n",
    "    ##   5 X 5 as the kernel size, \n",
    "    ##   relu as the activation function, and \n",
    "    ##   padding with zeros evenly to the surroundings of the input. \n",
    "    \n",
    "    layers.Conv2D(32, kernel_size=(5, 5), input_shape=(28, 28, 1), activation='relu', padding='same') , \n",
    "    \n",
    "        \n",
    "    ## A Max Pooling layer that has pooling window size and stride step both as 2\n",
    "    \n",
    "    layers.MaxPool2D(pool_size=2, strides=2),    \n",
    "    \n",
    "    ## A Convolutional layer that has * 64 filters *, relu as the activation function, and padding to let output have the same size as the input.\n",
    "    layers.Conv2D(64, (5, 5), activation='relu', padding='same'),\n",
    "    \n",
    "    ## A Max Pooling layer that has pooling window size and stride step both as 2\n",
    "    layers.MaxPool2D(pool_size=2, strides=2) ,\n",
    "    \n",
    "    ## A Flatten layer to flatten the input\n",
    "    layers.Flatten(),         \n",
    "\n",
    "    ## A fully connected layer with 128 neurons and an activation function of tanh \n",
    "    layers.Dense(128, activation='tanh'),  \n",
    "    ##-Q14: What is the number of output after applying above densely connected layer?    \n",
    "    \n",
    "    ## A fully connected layer with an activation function of Softmax that can output probability distribution for multiclass from 0 to 9\n",
    "    layers.Dense(10, activation='softmax')  \n",
    "\n",
    "])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "09b2fcb3",
   "metadata": {},
   "source": [
    "##### Display the architecture of your model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f99b7808",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_4 (Conv2D)           (None, 28, 28, 32)        832       \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 14, 14, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 14, 14, 64)        51264     \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 7, 7, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 3136)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               401536    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 454,922\n",
      "Trainable params: 454,922\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## You can use the following code to have a summary description on the architecture of the CNN model you build. \n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "783e6851",
   "metadata": {},
   "source": [
    "##### Compile the model\n",
    "\n",
    "Before the model is ready for training, it needs a few more settings. These are to be added during the [model's compiling](https://www.tensorflow.org/api_docs/python/tf/keras/Model) step:\n",
    "\n",
    "- [Optimizer](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers) : This is how the model is updated based on the data it sees and its loss function.\n",
    "- [Loss function](https://www.tensorflow.org/api_docs/python/tf/keras/losses) : This measures how accurate the model is during training. You want to minimize this function to \"steer\" the model in the right direction.\n",
    "- [Metrics](https://www.tensorflow.org/api_docs/python/tf/keras/metrics) : Used to monitor the training and testing steps.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b6fb8782",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    }
   ],
   "source": [
    "## Select an optimizer that implements the Adam algorithm, and validate to decide a proper learning rate for your model \n",
    "my_optimizer = optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=my_optimizer,\n",
    "    loss=keras.losses.sparse_categorical_crossentropy,\n",
    "    metrics=['accuracy', 'mse']\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "567e3703",
   "metadata": {},
   "source": [
    "## Fit/Train the model\n",
    "\n",
    "Train the model to fit the train dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3e029773",
   "metadata": {},
   "source": [
    "##### Obtain train dataset\n",
    "\n",
    "Apply the function previously defined for dataset preprocessing \n",
    "\n",
    "to Randomly sample 20,000 images and labels form the default train dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6f146eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "## set required number of samples to be randomly taken from the original train dataset\n",
    "train_sample_count = 20000\n",
    "\n",
    "train_batch_size = 100\n",
    " \n",
    "## Obtain the train dataset prepared using previously defined function\n",
    "dataset_train = preprocess_dataset(images_train, labels_train, train_sample_count, train_batch_size)      "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "10273401",
   "metadata": {},
   "source": [
    "##### Train your model\n",
    "Feed the train dataset to the model and let it  [fit to associate images and labels](https://www.tensorflow.org/api_docs/python/tf/keras/Model)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3f0d2fa6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "200/200 [==============================] - 4s 16ms/step - loss: 0.0882 - accuracy: 0.9732 - mse: 27.3981\n",
      "Epoch 2/15\n",
      "200/200 [==============================] - 3s 16ms/step - loss: 0.0891 - accuracy: 0.9729 - mse: 27.3982\n",
      "Epoch 3/15\n",
      "200/200 [==============================] - 3s 16ms/step - loss: 0.0848 - accuracy: 0.9754 - mse: 27.3981\n",
      "Epoch 4/15\n",
      "200/200 [==============================] - 3s 16ms/step - loss: 0.0844 - accuracy: 0.9739 - mse: 27.3982\n",
      "Epoch 5/15\n",
      "200/200 [==============================] - 3s 15ms/step - loss: 0.0836 - accuracy: 0.9736 - mse: 27.3981\n",
      "Epoch 6/15\n",
      "200/200 [==============================] - 3s 16ms/step - loss: 0.0838 - accuracy: 0.9747 - mse: 27.3983\n",
      "Epoch 7/15\n",
      "200/200 [==============================] - 3s 15ms/step - loss: 0.0816 - accuracy: 0.9755 - mse: 27.3984\n",
      "Epoch 8/15\n",
      "200/200 [==============================] - 3s 16ms/step - loss: 0.0767 - accuracy: 0.9761 - mse: 27.3987\n",
      "Epoch 9/15\n",
      "200/200 [==============================] - 3s 16ms/step - loss: 0.0774 - accuracy: 0.9761 - mse: 27.3988\n",
      "Epoch 10/15\n",
      "200/200 [==============================] - 3s 16ms/step - loss: 0.0744 - accuracy: 0.9780 - mse: 27.3986\n",
      "Epoch 11/15\n",
      "200/200 [==============================] - 3s 15ms/step - loss: 0.0707 - accuracy: 0.9790 - mse: 27.3987\n",
      "Epoch 12/15\n",
      "200/200 [==============================] - 3s 16ms/step - loss: 0.0699 - accuracy: 0.9795 - mse: 27.3988\n",
      "Epoch 13/15\n",
      "200/200 [==============================] - 3s 16ms/step - loss: 0.0685 - accuracy: 0.9801 - mse: 27.3989\n",
      "Epoch 14/15\n",
      "200/200 [==============================] - 4s 16ms/step - loss: 0.0692 - accuracy: 0.9796 - mse: 27.3989\n",
      "Epoch 15/15\n",
      "200/200 [==============================] - 3s 16ms/step - loss: 0.0669 - accuracy: 0.9794 - mse: 27.3989\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x28a48beb0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "epochs_count = 15\n",
    "## Train model\n",
    "model.fit(dataset_train, epochs=epochs_count)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "832515a4",
   "metadata": {},
   "source": [
    "## Tuning your Model\n",
    "\n",
    "Go back to adjust the values of hyperparameters such as `learning rate`, `batch size`, and `epochs` in your aboce codes, to improve and achieve an **accuracy more than 97%**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "40874051",
   "metadata": {},
   "source": [
    "## Evaluate/Test the trained model\n",
    "\n",
    "Evaluates your trained model based on the test dataset to see how the model performs on the test dataset after being trained to reach a good looking accuracy."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "accc735f",
   "metadata": {},
   "source": [
    "##### Obtain test dataset\n",
    "\n",
    "Apply the function previously defined for dataset preprocessing \n",
    "\n",
    "to randomly sample ALL number of images and labels form the `Original test dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "162ef7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set required number of samples to be randomly taken from the original test dataset\n",
    "test_sample_count = 10000\n",
    "\n",
    "## Obtain the test dataset prepared using previously defined function\n",
    "dataset_test = preprocess_dataset(images_test, labels_test, test_sample_count, test_sample_count)   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d2f3c728",
   "metadata": {},
   "source": [
    "##### evaluate your trained model\n",
    "Feed the obtained test dataset to the model and   [evaluate the accuracy](https://www.tensorflow.org/api_docs/python/tf/keras/Model)   on the test dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ce608f93",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 990ms/step - loss: 0.0811 - accuracy: 0.9747 - mse: 27.3378\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.08111119270324707, 0.9746999740600586, 27.33778190612793]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Evaluate model\n",
    "model.evaluate(dataset_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1999f6f6",
   "metadata": {},
   "source": [
    "## Make prediction "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "69d1a9c8",
   "metadata": {},
   "source": [
    "Pick one image from the MNIST dataset to see what digit your trained model predict, as it being classified to which of the 10 classes (0 to 9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ade29557",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pick the 2nd image from the original test images to feed into the trained model\n",
    "\n",
    "idx_img_predict = 1\n",
    "    \n",
    "image_predict = images_test[idx_img_predict]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "516a2136",
   "metadata": {},
   "source": [
    "Preporcess the picked image with the same procedure applied on train images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3ad198f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert image dimension to explicitly indicate it has only one sample and only one color channel\n",
    "image_predict = image_predict.reshape((1,) + image_predict.shape + (1,)) \n",
    "\n",
    "## Convert image pixel values to floating-point number and rescale to [0,1]\n",
    "image_predict = tf.cast(image_predict, tf.float32) / 255    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "918df7f8",
   "metadata": {},
   "source": [
    "Feed the preprocessed image to the trained model to   [predict a result](https://www.tensorflow.org/api_docs/python/tf/keras/Model). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cb200401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 626ms/step\n",
      "prediction result is: \n",
      " [[1.5925492e-06 2.0086480e-05 9.9997282e-01 4.3866055e-10 4.5734003e-12\n",
      "  9.7868783e-13 2.0403340e-07 4.0409920e-10 5.2571245e-06 5.4704005e-11]]\n"
     ]
    }
   ],
   "source": [
    "## Predict on the picked image \n",
    "prediction = model.predict(image_predict)\n",
    "##-Q25: What is the function name to fill in the ??? in the above code? \n",
    "\n",
    "print(\"prediction result is: \\n\", prediction)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eac529ad",
   "metadata": {},
   "source": [
    "[Interpret](https://numpy.org/doc/stable/reference/generated/numpy.argmax.html) the prediction by processing the multiclass result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "90d3b193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted digit number from this image is: \n",
      " 2\n"
     ]
    }
   ],
   "source": [
    "## Interpret the output from the Softmax activation function of the model to see what digit is predicted by your model \n",
    "print(\"The predicted digit number from this image is: \\n\", np.argmax(prediction))\n",
    "##-Q26: What is the function name to fill in the ??? in the above code? \n",
    "##      (hint, find the index that representing the class which has the highest predicted probability)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c4ca5618",
   "metadata": {},
   "source": [
    "##### Compare the prediction with the provided data\n",
    "\n",
    "Display the picked image of prediction to have a visual comparison "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8e8ce62b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2c455cc10>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Display the 2nd image from the original test images  \n",
    "plt.imshow(images_test[idx_img_predict])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9c22c298",
   "metadata": {},
   "source": [
    "Compare the provided label from MNIST dataset associated with this image of prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e21b49b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The provided label for this image is: \n",
      " 2\n"
     ]
    }
   ],
   "source": [
    "print(\"The provided label for this image is: \\n\", labels_test[idx_img_predict])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
